{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50fe24ca-0ddc-4f06-9d84-e59498bd07e5",
   "metadata": {},
   "source": [
    "## Computational Physics 2 (WS23/24) – Warm-up exercise\n",
    "\n",
    "**Deadline: 31.10.2023 at 23:59**\n",
    "\n",
    "Group: *Henri Poincaré *\n",
    "Students: *Max Heimann 580560, Lyding (Lukas) Brumm 604522, Burak Varol 623941*\n",
    "\n",
    "You will implement and test two algorithms: **conjugate gradient** and **power method**. We will see in a moment what they are useful for. Fill the notebook following the instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43203cce-f800-46be-b70c-4b53fafceda0",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "\n",
    "Here we load the needed libraries and we initialize the random number generator. **Important**: when using a random number generator, the seed needs to be set only once, preferebly at the beginning of the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "627c0eb1-6bce-40ec-976d-f1bb77c9e78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random.Generator(np.random.PCG64(12345))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a122e6da-eca2-4891-83f8-fb79815b7b06",
   "metadata": {},
   "source": [
    "### Positive-definite matrices\n",
    "\n",
    "Both algorithms will deal with hermitian positive-definite matrices. Recall:\n",
    "\n",
    "- Given a complex square matrix $A$, its hermitian conjugate $A^\\dagger$ is defined as its transposed complex-conjugated, i.e. $(A^\\dagger)_{ij} = (A_{ji})^*$.\n",
    "- A complex square matrix $A$ is said to be hermitian if $A=A^\\dagger$.\n",
    "- An hermitian matrix $A$ is said to be positive-definite if all its eigenvalues are positive.\n",
    "\n",
    "The following function generates and returns a random positive-definite matrix, along with its eigenvactors and eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a958716e-7597-4eea-9f04-4acf920cc824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function 'generate_positive_definite_matrix' contructs an NxN positive-definite matrix 'A',\n",
    "# its matrix of eigenvectors and its eigenvalues.\n",
    "#\n",
    "# Input parameters:\n",
    "#    N (integer)        : size of output matrix 'A'\n",
    "#    kappa (double)     : condition number of the output matrix 'A'\n",
    "#                         see https://en.wikipedia.org/wiki/Condition_number#Matrices\n",
    "# Output values: (A, U, evalues)\n",
    "#    A (np.matrix)      : positive-definite NxN matrix with condition number kappa\n",
    "#    U (np.matrix)      : NxN unitary matrix; each column of 'U' is an eigenvector of 'A'\n",
    "#    evalues (np.array) : N-component array with eigenvalues of 'A'\n",
    "\n",
    "def generate_positive_definite_matrix(N,kappa=10.):\n",
    "    assert isinstance(N, int) and N > 1 , \"N=\" + str(N) + \" must be an integer >= 2\"\n",
    "    assert isinstance(kappa, float) and kappa > 0. , \"kappa=\" + str(kappa) + \" must be a positive float\"\n",
    "    \n",
    "    rmat = np.asmatrix(rng.standard_normal(size=(N,N)) + 1j * rng.standard_normal(size=(N,N)))\n",
    "    U , _ = np.linalg.qr(rmat,mode='complete')\n",
    "    evalues = np.concatenate((1. + kappa*rng.random(N-2),[1.,kappa]))\n",
    "    D = np.asmatrix(np.diag(evalues))\n",
    "    A = np.matmul(np.matmul(U,D),U.getH())\n",
    "    \n",
    "    return A, U , evalues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6add5b3-4910-4a78-bb3f-92a71ff95671",
   "metadata": {},
   "source": [
    "### Power method\n",
    "\n",
    "Given a positive-definite matrix $A$, the power method allows to approximate its largest eigenvalue and the corresponding eigenvector with a certain specified tolerance $\\epsilon$. It is an iterative method: a number of steps are repeated cyclically, at each iteration one gets a better approximation of the eigenvalue and eigenvectors, the iteration is stopped when the approximation is good enough. It works as follows:\n",
    "\n",
    "1. Generate a random complex vector $v$ with norm equal to 1.\n",
    "2. Calculate $w=Av$ and $\\mu = \\| w \\|$.\n",
    "3. If $\\| w - \\mu v \\| < \\epsilon$, stop iteration and returns $\\mu$ and $v$ are eigenvalue and eigenvector.\n",
    "4. Replace $v \\leftarrow \\mu^{-1} w$ and repeat from 2.\n",
    "\n",
    "**Task:** Implement the power method within the function ```power_method```, with the following specifications.\n",
    "\n",
    "The *vector* $v$ is not necessarily a one-dimensional array, we want the flexibility to use more abstract vector spaces whose elements are generic $d$-dimensional arrays. In practice, the *vectors* $v$ must be implemented as ```numpy.ndarrays```. In this setup, the squared norm $\\|v\\|^2$ of the *vector* $v$ is given by the sum of the squared absolute value of all elements of $v$. Moreover, the *matrix* $A$ really needs to be thought as a linear function acting on the elements of the abstract vector space.\n",
    "\n",
    "```power_method``` must be a function that takes three inputs:\n",
    "- ```vshape``` is the shape of the elements $v$ of the abstract vector space;\n",
    "- ```apply_A``` is a function that takes the vector $v$ (represented as an instance of ```numpy.ndarrays```) and returns the vector $Av$ (represented as an instance of ```numpy.ndarrays``` with the same shape as $v$);\n",
    "- ```epsilon``` is the tolerance.\n",
    "\n",
    "```power_method``` must return:\n",
    "- the largest eignevalue $\\mu$;\n",
    "- the corresponding eigenvector (represented as an instance of ```numpy.ndarrays``` with the same shape as the input of ```apply_A```);\n",
    "- the number of iterations.\n",
    "\n",
    "A test function is provided below. Your implementation of ```power_method``` needs to pass this test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c17719c-c95b-4152-bce5-0c418fb64aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function 'power_method' calculates an approximation of the largest eigenvalue 'mu'\n",
    "# and corresponding eigenvector 'v' of the positive-definite linear map 'A'. The quality\n",
    "# of the approximation is dictated by the tolerance 'epsilon', in the sense that the\n",
    "# approximated eigenvalue and eigenvector satisfy\n",
    "#   | A(v) - mu*v | < epsilon\n",
    "#\n",
    "# The vectors over which 'A' acts are generically d-dimensional arrays. More precisely,\n",
    "# they are instances of 'numpy.ndarray' with shape 'vshape'.\n",
    "#\n",
    "# The linear map 'A' is indirectly provided as a function 'apply_A' which takes a vector\n",
    "# v and returns the vector A(v).\n",
    "#\n",
    "# Input parameters of power_method:\n",
    "#    vshape (tuple of ints) : shape of the arrays over which 'A' acts\n",
    "#    apply_A (function)     : function v -> A(v)\n",
    "#    epsilon (float)        : tolerance\n",
    "# Output values: (mu, v, niters)\n",
    "#    mu (float)             : largest eigenvalue of A\n",
    "#    v (numpy.ndarray)      : corresponding eigenvector\n",
    "#    niters (int)           : number of iterations\n",
    "\n",
    "def power_method(vshape,apply_A,epsilon):\n",
    "    \"\"\"Calculate the biggest eigenvalue of an Hermitian Operator a \n",
    "\n",
    "    Args:\n",
    "        vshape (tuple of ints) : shape of the arrays over which 'A' acts\n",
    "        apply_A (function)     : function v -> A(v)\n",
    "        epsilon (float)        : tolerance\n",
    "\n",
    "    Returns:\n",
    "        mu (float)             : largest eigenvalue of A\n",
    "        v (numpy.ndarray)      : corresponding eigenvector\n",
    "        niters (int)           : number of iterations\n",
    "    \"\"\"\n",
    "    assert callable(apply_A) , \"apply_A must be a function\"\n",
    "    assert isinstance(epsilon, float) and epsilon > 0. , \"epsilon=\" + str(epsilon) + \" must be a positive float\"\n",
    "    assert isinstance(vshape,tuple) , \"vshape must be a tuple\"\n",
    "    esqr = epsilon**2\n",
    "    vinit = np.random.random_sample(vshape)\n",
    "    norm = np.linalg.norm(vinit) # same as np.sqrt(sum(vinit**2)) but probably faster\n",
    "    v = vinit/norm\n",
    "    Av = apply_A(v)\n",
    "    mu = np.linalg.norm(Av)\n",
    "    res = Av-v*mu\n",
    "    niters = 0\n",
    "    while esqr < np.real(np.vdot(res,res)):\n",
    "        v = Av/mu\n",
    "        mu = np.linalg.norm(v)\n",
    "        Av = apply_A(v)\n",
    "        res = Av-v*mu\n",
    "        niters+=1\n",
    "    v=v/mu\n",
    "    return mu, v, niters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca841b5-ff0a-4241-a225-a024406d5200",
   "metadata": {},
   "source": [
    "#### Test\n",
    "\n",
    "Run the following cell. If the power method is correctly implemented, then the test will pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "568d3b9a-b4de-4dc5-9c99-767c3dbb919b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape =  (4,) \tresidue =  9.637436543930886e-10 \titerations =  622 \tTest passes:  True\n",
      "shape =  (1, 5) \tresidue =  9.955871969088826e-14 \titerations =  1152 \tTest passes:  True\n",
      "shape =  (3, 2, 4) \tresidue =  9.20675726347073e-10 \titerations =  663 \tTest passes:  True\n",
      "shape =  (5, 2) \tresidue =  9.853133957728112e-14 \titerations =  1642 \tTest passes:  True\n"
     ]
    }
   ],
   "source": [
    "def test_power_method():\n",
    "\n",
    "    def test_engine(shape,epsilon):\n",
    "        \n",
    "        N = int(np.prod(shape))\n",
    "        A , _ , _ = generate_positive_definite_matrix(N)\n",
    "        \n",
    "        def apply_A(v):\n",
    "            assert isinstance(v,np.ndarray) , \"v must be an np.ndarray\"\n",
    "            assert v.shape==shape , \"v has shape \"+str(v.shape)+\", it must have shape \"+str(shape)\n",
    "            return np.asarray(np.dot(A,v.flatten())).reshape(shape)\n",
    "        \n",
    "        mu , v , niters = power_method(shape,apply_A,epsilon)\n",
    "        delta = apply_A(v) - mu*v\n",
    "        res = np.sqrt(np.vdot(delta,delta).real)\n",
    "        print(\"shape = \" , shape , \"\\tresidue = \" , res , \"\\titerations = \" , niters , \"\\tTest passes: \" , res<=epsilon)\n",
    "    \n",
    "    \n",
    "    test_engine((4,),1.e-8)\n",
    "    test_engine((1,5),1.e-12)\n",
    "    test_engine((3,2,4),1.e-8)\n",
    "    test_engine((5,2),1.e-12)\n",
    "\n",
    "test_power_method()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4eac907-2458-4974-a92e-384716aa2e76",
   "metadata": {},
   "source": [
    "### Conjugate gradient\n",
    "\n",
    "**Task.**\n",
    "1. Read about the conjugate gradient on Wikipedia.\n",
    "2. Implement the conjugate gradient, using same conventions as for the power method.\n",
    "3. Write a description of the algorithm here (in the same spirit as the description of the power method).\n",
    "4. Run and pass the test provided below.\n",
    "5. Discuss intermediate steps with tutors.\n",
    "\n",
    "# Conjugate Gradient Algorithm\n",
    "\n",
    "The Conjugate Gradient (CG) algorithm is an iterative method used for solving systems of linear equations. It's goal is to calculate the solution to the equation $ \\bold{A}\\vec{x} = \\vec{b}$, $\\bold{A}^{-1}\\vec{b}$.  It's particularly well-suited for symmetric positive-definite matrices, as well as hermitian matricies. \n",
    "\n",
    "## Algorithm Steps:\n",
    "\n",
    "1. **Initialization**:\n",
    "   - Choose an initial guess for the solution, denoted as $\\vec{x}_0$.\n",
    "   - Initialize the residual $\\vec{r}_0 = \\bold{A}\\vec{x}_0 - \\vec{b}$  \n",
    "   - Set the search direction $\\vec{d}_0 = \\vec{r}_0$.\n",
    "\n",
    "2. **Iterate**:\n",
    "   - For each iteration (k = 0, 1, 2, ...):\n",
    "      - Compute $\\vec{z} = \\bold{A}\\vec{d}_0$ and to save computations later on\n",
    "      - Compute the step size $\\alpha _k = \\frac{\\vec{r}^{\\dagger}_{k}\\vec{r}_{k}}{\\vec{d}^{\\dagger}_k\\vec{z}}$\n",
    "         - this minimizes the residual in the search direction\n",
    "      - Update the solution: $\\vec{x}_{k+1} = \\vec{x}_{k} + \\alpha _k \\vec{d}_k$.\n",
    "      - Update the residual: $\\vec{r}_{k+1} = \\vec{r}_k - \\alpha _k \\vec{z}$.\n",
    "      - Compute the next search direction using the new residual: $\\beta _k = \\frac{\\vec{r}^{\\dagger}_{k}\\vec{r}_{k}}{\\vec{r}^{\\dagger}_{k+1}\\vec{r}_{k+1}}$, $\\vec{d}_{k+1} = \\vec{r}_{k+1} + \\beta _k \\vec{d}_k$.\n",
    "\n",
    "3. **Termination**:\n",
    "   - Repeat the iterations until a convergence criterion is met, such as reaching a specified tolerance or a maximum number of iterations.\n",
    "\n",
    "### Additional Comments on implementation\n",
    "The code below is not a 1-to-1 implementation of the algorithm above. For example the value of $\\vec{r}^{\\dagger}_{k}\\vec{r}_{k}$ is stored in a variable since it appears in multiple steps.\n",
    "\n",
    "A second difference is the implementation of the \"harder\" convergence criterion in a if statement. This is implemented to circumvent the accumulation of floating point error in the residual: $|\\vec{r}|$ might be smaller than $\\epsilon$, however since it is computed iteratively, accumulating floating point error might occur. To make sure this error won't be higher than the actually wanted precission, the r value is recalculated every 100th loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "df2d2a0d-cd78-47b6-b509-fee1f8c07c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function 'conjugate_gradient' calculates an approximation 'x' of 'A^{-1}(b)', where\n",
    "# 'A' is a positive-definite linear map 'A', and 'b' is a vector in the domain of 'A'.\n",
    "# The quality of the approximation is dictated by the tolerance 'epsilon', in the sense\n",
    "# that the following inequality is satisfied\n",
    "#   | A(x) - b | <= epsilon |b|\n",
    "#\n",
    "# The vectors over which 'A' acts are generically d-dimensional arrays. More precisely,\n",
    "# they are instances of 'numpy.ndarray' with shape 'vshape'.\n",
    "#\n",
    "# The linear map 'A' is indirectly provided as a function 'apply_A' which takes a vector\n",
    "# v and returns the vector A(v).\n",
    "#\n",
    "# Input parameters of power_method:\n",
    "#    apply_A (function)     : function v -> A(v)\n",
    "#    b (numpy.ndarray)      : vector 'b'\n",
    "#    epsilon (float)        : tolerance\n",
    "# Output values: (x, niters)\n",
    "#    x (numpy.ndarray)      : approximation of 'A^{-1}(b)'\n",
    "#    niters (int)           : number of iterations\n",
    "\n",
    "def conjugate_gradient(apply_A,b,epsilon):\n",
    "    \"\"\"Calculate A^-1b with the conjugate gradient method\n",
    "\n",
    "    Args:\n",
    "        apply_A (function)     : function v -> A(v)\n",
    "        b (numpy.ndarray)      : vector 'b'\n",
    "        epsilon (float)        : tolerance\n",
    "\n",
    "    Returns:\n",
    "        x (numpy.ndarray)      : approximation of 'A^{-1}(b)'\n",
    "        niters (int)           : number of iterations\n",
    "    \"\"\"\n",
    "    assert callable(apply_A) , \"apply_A must be a function\"\n",
    "    assert isinstance(epsilon, float) and epsilon > 0. , \"epsilon=\" + str(epsilon) + \" must be a positive float\"\n",
    "    assert isinstance(b,np.ndarray) , f\"b={b} must be an np.ndarray\"\n",
    "    x = np.random.random_sample(b.shape)\n",
    "    r = b - apply_A(x)\n",
    "    d = r\n",
    "    niters = 0\n",
    "    rsqr = np.real(np.vdot(r,r))\n",
    "    esqr = epsilon**2\n",
    "    \n",
    "    while (rsqr > esqr):\n",
    "        z = apply_A(d)\n",
    "        a = rsqr/np.real(np.vdot(d,z))\n",
    "        x = x + a*d\n",
    "        if niters % 2 == 1:\n",
    "            r = -apply_A(x) + b \n",
    "            d = r\n",
    "        else:\n",
    "            r = r-a*z\n",
    "        rsqr_old = rsqr\n",
    "        rsqr = np.real(np.vdot(r,r))\n",
    "        beta = rsqr/rsqr_old\n",
    "        d = r + beta*d\n",
    "        niters += 1\n",
    "    return x, niters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2ad99aa0-b80d-4cdb-a1a1-933b978de93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape =  (4,) \tresidue =  5.440918002156144e-09 \titerations =  52 \tTest passes:  True\n",
      "shape =  (1, 5) \tresidue =  7.385550204290278e-13 \titerations =  77 \tTest passes:  True\n",
      "shape =  (3, 2, 4) \tresidue =  6.6786408135607885e-09 \titerations =  50 \tTest passes:  True\n",
      "shape =  (5, 2) \tresidue =  7.781023332422448e-13 \titerations =  75 \tTest passes:  True\n"
     ]
    }
   ],
   "source": [
    "def test_conjugate_gradient():\n",
    "\n",
    "    def test_engine(shape,epsilon):\n",
    "        \n",
    "        N = int(np.prod(shape))\n",
    "        A , _ , _ = generate_positive_definite_matrix(N)\n",
    "        b = rng.standard_normal(size=shape) + 1j * rng.standard_normal(size=shape)\n",
    "        \n",
    "        def apply_A(v):\n",
    "            assert isinstance(v,np.ndarray) , \"v must be an np.ndarray\"\n",
    "            assert v.shape==shape , \"v has shape \"+str(v.shape)+\", it must have shape \"+str(shape)\n",
    "            return np.asarray(np.dot(A,v.flatten())).reshape(shape)\n",
    "        \n",
    "        x , niters = conjugate_gradient(apply_A,b,epsilon)\n",
    "        delta = apply_A(x) - b\n",
    "        res = np.sqrt(np.vdot(delta,delta).real)\n",
    "        print(\"shape = \" , shape , \"\\tresidue = \" , res , \"\\titerations = \" , niters , \"\\tTest passes: \" , res<=epsilon*np.sqrt(np.vdot(b,b).real))\n",
    "    \n",
    "    \n",
    "    test_engine((4,),1.e-8)\n",
    "    test_engine((1,5),1.e-12)\n",
    "    test_engine((3,2,4),1.e-8)\n",
    "    test_engine((5,2),1.e-12)\n",
    "\n",
    "test_conjugate_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdba46b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0857b228",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
